# SEPS框架可视化流程图

## 1. 整体架构鸟瞰图

```
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃                    SEPS: Semantic-Enhanced Patch Slimming         ┃
┃                    语义增强的Patch精简框架                          ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

                            输入数据流
     ┌──────────────┬──────────────┬──────────────────┐
     │              │              │                  │
     ▼              ▼              ▼                  │
┌─────────┐  ┌─────────┐  ┌───────────────┐         │
│  Image  │  │ Sparse  │  │ Dense Text    │         │
│224x224  │  │  Text   │  │(MLLM生成)     │         │
│or 384   │  │  (原始)  │  │               │         │
└────┬────┘  └────┬────┘  └───────┬───────┘         │
     │            │                │                 │
     ▼            ▼                ▼                 │
┌─────────┐  ┌─────────┐  ┌───────────────┐         │
│   ViT   │  │  BERT   │  │     BERT      │         │
│  Swin   │  │ Encoder │  │   Encoder     │         │
└────┬────┘  └────┬────┘  └───────┬───────┘         │
     │            │                │                 │
     ▼            ▼                ▼                 │
(B,N+1,C)    (B,Ls,C)        (B,Ld,C)               │
img_embs     cap_embs      long_cap_embs            │
     │            │                │                 │
     └────────────┴────────────────┘                 │
                  │                                  │
                  ▼                                  │
     ╔════════════════════════════════╗              │
     ║   L2 Normalization & [CLS]    ║              │
     ║         Feature Split          ║              │
     ╚════════════════════════════════╝              │
                  │                                  │
                  ▼                                  │
     ┌────────────────────────────────┐              │
     │    Spatial Patches (B,N,C)     │              │
     └────────────────────────────────┘              │
                  │                                  │
         ┌────────┴────────┐                         │
         │                 │                         │
         ▼                 ▼                         │
┏━━━━━━━━━━━━━┓   ┏━━━━━━━━━━━━━━━━┓                │
┃  s^{im}     ┃   ┃ s^{st}, s^{dt} ┃                │
┃ 图像自注意力  ┃   ┃  交叉注意力      ┃                │
┗━━━━━━━━━━━━━┛   ┗━━━━━━━━━━━━━━━━┛                │
         │                 │                         │
         └────────┬────────┘                         │
                  ▼                                  │
     ╔════════════════════════════════╗              │
     ║         SDTPS Module           ║              │
     ║  Sparse & Dense Text-Aware     ║              │
     ║      Patch Selection           ║              │
     ╚════════════════════════════════╝              │
                  │                                  │
         ┌────────┴────────┐                         │
         │                 │                         │
         ▼                 ▼                         │
  ┌──────────────┐  ┌──────────────┐                │
  │ TokenSparse  │  │ TokenAggr    │                │
  │ 显著性评分    │  │ Patch聚合     │                │
  │ (B,N,C)→(B,K,C) │ (B,K,C)→(B,Nc,C)│               │
  └──────────────┘  └──────────────┘                │
                  │                                  │
                  ▼                                  │
           (B,Nc+2,C)                                │
         Final Tokens                                │
                  │                                  │
                  ▼                                  │
     ╔════════════════════════════════╗              │
     ║         HRPA Module            ║              │
     ║ Highly-Relevant Patch-Word     ║              │
     ║        Alignment               ║              │
     ╚════════════════════════════════╝              │
                  │                                  │
                  ▼                                  │
     Similarity Matrix (B_v, B_t)                    │
                  │                                  │
                  ▼                                  │
     ╔════════════════════════════════╗              │
     ║        Loss Function           ║              │
     ║  L = L_align + λ·L_ratio       ║              │
     ╚════════════════════════════════╝              │
                  │                                  │
                  ▼                                  │
         Gradient Backprop                           │
                  │                                  │
                  └──────────────────────────────────┘
                         Model Update
```

---

## 2. SDTPS模块详细流程

### 2.1 TokenSparse: 语义评分与选择

```
输入: img_patches (B, N, C) - N个image patch
     s_im (B, N) - 图像自注意力
     s_st (B, N) - 稀疏文本注意力
     s_dt (B, N) - 稠密文本注意力 [论文版本]

┌─────────────────────────────────────────────────────────────┐
│                  Step 1: 计算综合得分                         │
└─────────────────────────────────────────────────────────────┘

  论文版本 (公式1-3):                    开源代码版本:
  ┌──────────────────┐                   ┌──────────────┐
  │    patches       │                   │   s_im       │
  │   (B, N, C)      │                   │   (B, N)     │
  └────────┬─────────┘                   └──────┬───────┘
           │                                    │
           ▼                                    │
  ┌──────────────────┐                         │
  │  score_predictor │                         │
  │  MLP: C→C/4→1    │                         │
  │  + Sigmoid       │                         │
  └────────┬─────────┘                         │
           │                                    │
           ▼                                    │
       s_pred (B,N)                             │
           │                                    │
  ┌────────┴─────────┐                          │
  │  Normalize:      │                          │
  │  s_im → [0,1]    │                          │
  │  s_st → [0,1]    │                   ┌──────┴───────┐
  │  s_dt → [0,1]    │                   │   s_st       │
  └────────┬─────────┘                   │   (B, N)     │
           │                             └──────┬───────┘
           ▼                                    │
  score = (1-2β)·s_pred                         │
        + β·(s_st + s_dt + 2·s_im)              │
        (B, N)                                  │
           │                                    │
           └────────────────┬───────────────────┘
                           │
                           ▼
                 综合得分 score (B, N)

┌─────────────────────────────────────────────────────────────┐
│                  Step 2: Top-K选择                           │
└─────────────────────────────────────────────────────────────┘

    score (B, N)
        │
        ▼
   torch.sort(descending=True)
        │
        ├─────────────────┬─────────────────┐
        ▼                 ▼                 ▼
   score_sort         score_index     keep_indices
     (B, N)              (B, N)          (B, K)
                                         K=⌈N×ρ⌉
        │                                   │
        └───────────────┬───────────────────┘
                        ▼
              torch.gather(patches)
                        │
        ┌───────────────┴───────────────┐
        │                               │
        ▼                               ▼
  select_tokens                    non_tokens
    (B, K, C)                      (B, N-K, C)
        │                               │
        │                               ▼
        │                      weighted_sum by score
        │                               │
        │                               ▼
        │                         extra_token
        │                           (B, 1, C)
        │                               │
        └───────────────┬───────────────┘
                        │
                        ▼
            score_mask (B, N) - 决策矩阵D
            1=selected, 0=discarded

输出:
  - select_tokens: (B, K, C) - 选中的显著patch
  - extra_token: (B, 1, C) - 融合的冗余patch
  - score_mask: (B, N) - 训练用决策矩阵
```

### 2.2 TokenAggregation: Patch聚合

```
输入: select_tokens (B, K, C)
目标: 聚合为 N_c 个紧凑patch, N_c = ⌈N×ρ×aggr_ratio⌉

┌─────────────────────────────────────────────────────────────┐
│            论文版本: 双分支联合聚合 (公式4)                     │
└─────────────────────────────────────────────────────────────┘

稀疏文本分支                         稠密文本分支
─────────────                       ─────────────
select_tokens_sparse                select_tokens_dense
    (B, K, C)                           (B, K, C)
        │                                   │
        ▼                                   ▼
┌─────────────────┐                 ┌─────────────────┐
│ LayerNorm       │                 │ LayerNorm       │
│ Linear(C→hidden)│                 │ Linear(C→hidden)│
│ GELU            │                 │ GELU            │
│ Linear(hidden→Nc)│                │ Linear(hidden→Nc)│
└────────┬────────┘                 └────────┬────────┘
         │                                   │
         ▼                                   ▼
    (B, K, Nc)                          (B, K, Nc)
         │                                   │
         ▼                                   ▼
   transpose                             transpose
         │                                   │
         ▼                                   ▼
    (B, Nc, K)                          (B, Nc, K)
         │                                   │
         ▼                                   ▼
    softmax(dim=2)                      softmax(dim=2)
         │                                   │
         ▼                                   ▼
       W_s                                  W_d
   Σ_k W[j,k]=1                         Σ_k W[j,k]=1
         │                                   │
         ▼                                   ▼
  W_s @ V_s                             W_d @ V_d
    (B,Nc,C)                              (B,Nc,C)
         │                                   │
         └──────────────┬────────────────────┘
                        ▼
                  aggr_tokens
                   (B, Nc, C)

┌─────────────────────────────────────────────────────────────┐
│            开源版本: 单一聚合网络                               │
└─────────────────────────────────────────────────────────────┘

   select_tokens (B, K, C)
         │
         ▼
   TokenAggregation
   (单一MLP网络)
         │
         ▼
   aggr_tokens (B, Nc, C)

   稀疏和稠密分支各自独立聚合

┌─────────────────────────────────────────────────────────────┐
│                  最终Token拼接                                │
└─────────────────────────────────────────────────────────────┘

   ┌──────────┐   ┌──────────┐   ┌──────────┐
   │   [CLS]  │ + │  aggr    │ + │  extra   │
   │  (B,1,C) │   │ (B,Nc,C) │   │ (B,1,C)  │
   └────┬─────┘   └────┬─────┘   └────┬─────┘
        └──────────────┴──────────────┘
                      │
                      ▼
              final_tokens
               (B, Nc+2, C)
                      │
                      ▼
              L2 Normalize
```

---

## 3. HRPA模块: 高相关性Patch-Word对齐

```
输入:
  - patch_features: (B_v, Nc+2, C) [L2归一化]
  - word_features: (B_v, M, C) [L2归一化]

┌─────────────────────────────────────────────────────────────┐
│           Step 1: 计算Patch-Word相似度矩阵                     │
└─────────────────────────────────────────────────────────────┘

   patch_features              word_features
     (B_v, Nc+2, C)              (B_v, M, C)
           │                         │
           └──────────┬──────────────┘
                      ▼
              torch.bmm(words, patches^T)
                      │
                      ▼
          A = patch-word similarity
             (B_v, M, Nc+2)
                      │
         ┌────────────┴────────────┐
         │                         │
    论文版本                   开源版本
         │                         │
         ▼                         ▼
    纯余弦相似度                LeakyReLU(A, 0.1)
    A ∈ [-1,1]                  抑制负相似度
         │                         │
         └────────────┬────────────┘
                      │
                      ▼
              相似度矩阵 A
             (B_v, M, Nc+2)

┌─────────────────────────────────────────────────────────────┐
│           Step 2: 双向对齐评分                                │
└─────────────────────────────────────────────────────────────┘

  ┌──────────────────────────────────────────────────────┐
  │         word→patch对齐 (Text-to-Image)               │
  └──────────────────────────────────────────────────────┘

        A (B_v, M, Nc+2)
              │
              ▼
       max(dim=patch)  ← 每个word找最相关patch
              │
              ▼
        row_sim (B_v, M)
              │
      ┌───────┴──────────┐
      │                  │
      ▼                  ▼
    mean            [论文] topk(5)
      │                  │
      ▼                  ▼
  row_mean          row_topk
   (B_v,1)           (B_v,5)
      │                  │
      │                  ▼
      │             MLP(topk)
      │                  │
      │                  ▼
      │             row_mlp
      │              (B_v,1)
      └──────┬───────────┘
             │
             ▼
       score_word2patch
          (B_v, 1)

  ┌──────────────────────────────────────────────────────┐
  │         patch→word对齐 (Image-to-Text)               │
  └──────────────────────────────────────────────────────┘

        A (B_v, M, Nc+2)
              │
              ▼
       max(dim=word)  ← 每个patch找最相关word
              │
              ▼
        col_sim (B_v, Nc+2)
              │
      ┌───────┴──────────┐
      │                  │
      ▼                  ▼
    mean            [论文] topk(5)
      │                  │
      ▼                  ▼
   col_mean          col_topk
   (B_v,1)           (B_v,5)
      │                  │
      │                  ▼
      │             MLP(topk)
      │                  │
      │                  ▼
      │             col_mlp
      │              (B_v,1)
      └──────┬───────────┘
             │
             ▼
       score_patch2word
          (B_v, 1)

┌─────────────────────────────────────────────────────────────┐
│           Step 3: 组合相似度                                  │
└─────────────────────────────────────────────────────────────┘

   score_word2patch + score_patch2word
          (B_v,1)          (B_v,1)
                │
                ▼
            S(I, T_i)
             (B_v, 1)
        图像-第i个文本的相似度
```

---

## 4. 特征融合流程对比

### 4.1 论文版本: 深度融合

```
┌─────────────────────────────────────────────────────────────┐
│               稀疏文本 + 稠密文本 深度融合                       │
└─────────────────────────────────────────────────────────────┘

        img_patches (B, N, C)
              │
    ┌─────────┴─────────┐
    │                   │
    ▼                   ▼
  s^{st}              s^{dt}
稀疏文本注意力         稠密文本注意力
    │                   │
    └─────────┬─────────┘
              │
              ▼
    综合得分 = (1-2β)·s_pred
             + β·(s_st + s_dt + 2·s_im)
              │
    ┌─────────┴─────────┐
    │                   │
    ▼                   ▼
 select_sparse      select_dense
  (B, K, C)          (B, K, C)
    │                   │
    ▼                   ▼
  W_s @ V_s          W_d @ V_d
    │                   │
    └─────────┬─────────┘
              │
              ▼
        aggr_tokens
         (B, Nc, C)
              │
              ▼
    [CLS] + aggr + extra
              │
              ▼
        final_tokens
         (B, Nc+2, C)
              │
              ▼
      HRPA(final, sparse_text)
              │
              ▼
           S(I,T)
```

### 4.2 开源版本: 独立双分支

```
┌─────────────────────────────────────────────────────────────┐
│                    稀疏分支 (独立)                            │
└─────────────────────────────────────────────────────────────┘

    img_patches (B, N, C)
          │
          ▼
    s_st = cap_glo · patches
          │
          ▼
    score = s_im + s_st
          │
          ▼
    select_tokens_cap
        (B, K, C)
          │
          ▼
    TokenAggregation
          │
          ▼
    aggr_tokens_cap
        (B, Nc, C)
          │
          ▼
    [CLS] + aggr + extra_cap
          │
          ▼
    final_tokens_cap
        (B, Nc+2, C)
          │
          ▼
    HRPA(final, cap_embs)
          │
          ▼
       sim_sparse
        (B_v, 1)

┌─────────────────────────────────────────────────────────────┐
│                    稠密分支 (独立)                            │
└─────────────────────────────────────────────────────────────┘

    img_patches (B, N, C)
          │
          ▼
    s_dt = long_cap_glo · patches
          │
          ▼
    score = s_im + s_dt
          │
          ▼
    select_tokens_long
        (B, K, C)
          │
          ▼
    TokenAggregation
          │
          ▼
    aggr_tokens_long
        (B, Nc, C)
          │
          ▼
    [CLS] + aggr + extra_long
          │
          ▼
    final_tokens_long
        (B, Nc+2, C)
          │
          ▼
    HRPA(final, long_cap_embs)
          │
          ▼
       sim_dense
        (B_v, 1)

┌─────────────────────────────────────────────────────────────┐
│                      相似度融合                               │
└─────────────────────────────────────────────────────────────┘

    sim_sparse + sim_dense
        │
        ▼
     final_sim
      (B_v, 1)
```

---

## 5. 损失函数计算流程

```
┌─────────────────────────────────────────────────────────────┐
│                   相似度矩阵组装                               │
└─────────────────────────────────────────────────────────────┘

对所有B_t个文本循环:
    for i in range(B_t):
        S(I, T_i) → (B_v, 1)

组装:
    sims = [S(I,T_0), S(I,T_1), ..., S(I,T_{B_t-1})]
         ↓ concat
    sims (B_v, B_t)

┌─────────────────────────────────────────────────────────────┐
│              对比损失 L_align (公式6)                          │
└─────────────────────────────────────────────────────────────┘

        sims (B_v, B_t)
             │
             ▼
    ┌────────────────────┐
    │   diagonal = diag  │  ← 正样本对相似度
    │      (B,)          │
    └─────────┬──────────┘
              │
      ┌───────┴───────┐
      │               │
      ▼               ▼
    d1 (B,B)       d2 (B,B)
   行扩展          列扩展
      │               │
      └───────┬───────┘
              │
    ┌─────────┴──────────┐
    │                    │
    ▼                    ▼
 Image→Text          Text→Image
  检索损失              检索损失
    │                    │
cost_i2t            cost_t2i
  (B,B)               (B,B)
    │                    │
    ▼                    ▼
[α-S(I,T)+S(I,T̂)]_+ [α-S(I,T)+S(Î,T)]_+
    │                    │
    ▼                    ▼
  masked_fill         masked_fill
(屏蔽正样本)          (屏蔽正样本)
    │                    │
    ▼                    ▼
 max (hard)          max (hard)
  (B,)                (B,)
    │                    │
    └─────────┬──────────┘
              │
              ▼
    L_align = Σ(cost_i2t) + Σ(cost_t2i)
                 scalar

┌─────────────────────────────────────────────────────────────┐
│              比例约束损失 L_ratio (公式7)                       │
└─────────────────────────────────────────────────────────────┘

论文版本:                       开源版本:
────────                        ────────
D_s (B_t,B_v,N) - 稀疏分支        score_mask_all
D_d (B_t,B_v,N) - 稠密分支        = D_s + D_d
      │                             │
      ▼                             ▼
sparse_ratio = mean(D_s)      actual_ratio = mean(score_mask)
dense_ratio = mean(D_d)             │
      │                             │
      ▼                             ▼
L_ratio =                     L_ratio =
(sparse_ratio - ρ)²           (actual_ratio - ρ)²
+ (dense_ratio - ρ)²
      │                             │
      └──────────┬──────────────────┘
                 │
                 ▼
             L_ratio
              scalar

┌─────────────────────────────────────────────────────────────┐
│                    总损失                                     │
└─────────────────────────────────────────────────────────────┘

    L_align + ratio_weight × L_ratio
       │
       ▼
     Total Loss
       │
       ▼
    backward()
```

---

## 6. 数据流示例 (ViT-Base-224)

```
┌─────────────────────────────────────────────────────────────┐
│                    输入阶段                                   │
└─────────────────────────────────────────────────────────────┘

Batch: B = 32

Image:                  (32, 3, 224, 224)
    ↓ ViT-Base
ViT Output:             (32, 197, 768)
    ↓ Linear Proj
img_embs:               (32, 197, 512)

Sparse Text:            (32, 30)  # 约30个词
    ↓ BERT-Base
cap_embs:               (32, 30, 512)

Dense Text:             (32, 200)  # 约200个词
    ↓ BERT-Base
long_cap_embs:          (32, 200, 512)

┌─────────────────────────────────────────────────────────────┐
│                    处理阶段                                   │
└─────────────────────────────────────────────────────────────┘

L2归一化:
img_patches:            (32, 196, 512)  # 去掉[CLS]
cap_embs_norm:          (32, 30, 512)
long_cap_embs_norm:     (32, 200, 512)

注意力计算 (对第i个text):
s_im:                   (32, 196)
s_st:                   (32, 196)
s_dt:                   (32, 196)

TokenSparse (sparse_ratio=0.5):
K = ⌈196 × 0.5⌉ = 98
select_tokens:          (32, 98, 512)
extra_token:            (32, 1, 512)
score_mask:             (32, 196)

TokenAggregation (aggr_ratio=0.4):
N_c = ⌈196 × 0.5 × 0.4⌉ = 39
aggr_tokens:            (32, 39, 512)

拼接:
final_tokens:           (32, 41, 512)
                        = [CLS](1) + aggr(39) + extra(1)

HRPA (对第i个text):
A:                      (32, 30, 41)  # 相似度矩阵
row_sim:                (32, 30)
col_sim:                (32, 41)
S(I,T_i):               (32, 1)

循环所有32个文本:
sims:                   (32, 32)  # B_v=B_t=32

┌─────────────────────────────────────────────────────────────┐
│                    损失阶段                                   │
└─────────────────────────────────────────────────────────────┘

对比损失:
diagonal:               (32,)
cost_i2t:               (32, 32) → max → (32,)
cost_t2i:               (32, 32) → max → (32,)
L_align:                scalar

比例损失:
score_mask_all:         (32, 32, 196)
actual_ratio:           scalar ≈ 0.5
L_ratio:                scalar

总损失:
L = L_align + 2.0 × L_ratio

┌─────────────────────────────────────────────────────────────┐
│                  内存占用估算                                  │
└─────────────────────────────────────────────────────────────┘

img_embs:               32×197×512×4B = 12.9MB
cap_embs:               32×30×512×4B = 2.0MB
long_cap_embs:          32×200×512×4B = 13.1MB
select_tokens:          32×98×512×4B = 6.4MB
aggr_tokens:            32×39×512×4B = 2.6MB
相似度矩阵A:             32×30×41×4B = 0.16MB
sims:                   32×32×4B = 0.004MB

总激活值 (不含梯度):     ~40MB
训练峰值 (含梯度+优化器):  ~500MB
```

---

## 7. 关键差异对比流程图

```
┌─────────────────────────────────────────────────────────────┐
│          评分机制: 论文 vs 开源代码                            │
└─────────────────────────────────────────────────────────────┘

论文公式(3):                    开源代码:
━━━━━━━━━━━━                    ━━━━━━━━

    patches                        s_im
       │                            │
       ▼                            │
   MLP预测                          │
       │                            │
       ▼                            │
    s_pred                          │
       │                            │
  ┌────┴────┐                       │
  │         │                       │
  ▼         ▼                       │
s_st      s_dt                    s_st
  │         │                       │
  └────┬────┘                       │
       │                            │
 ┌─────┴─────┐                      │
 │  Normalize│                      │
 └─────┬─────┘                      │
       │                            │
  score =                           │
  (1-2β)·s_pred                     │
  +β·(s_st+s_dt+2·s_im)             │
       │                            │
       └────────────┬───────────────┘
                    │
                    ▼
               Final Score

┌─────────────────────────────────────────────────────────────┐
│          聚合方式: 论文 vs 开源代码                            │
└─────────────────────────────────────────────────────────────┘

论文公式(4):                    开源代码:
━━━━━━━━━━━━                    ━━━━━━━━

V_s          V_d                V_s      V_d
 │            │                  │        │
 ▼            ▼                  ▼        ▼
MLP_s       MLP_d              MLP      MLP
 │            │                  │        │
 ▼            ▼                  ▼        ▼
W_s          W_d              aggr_s   aggr_d
 │            │                  │        │
 └─────┬──────┘                  └───┬────┘
       │                             │
       ▼                             ▼
 W_s@V_s + W_d@V_d           HRPA(aggr_s, cap)
       │                     + HRPA(aggr_d, long_cap)
       │                             │
       ▼                             │
   aggr_tokens                       │
       │                             │
       └──────────┬──────────────────┘
                  │
                  ▼
            Final Aggregation

┌─────────────────────────────────────────────────────────────┐
│          HRPA增强: 论文 vs 开源代码                            │
└─────────────────────────────────────────────────────────────┘

论文公式(5):                    开源代码:
━━━━━━━━━━━━                    ━━━━━━━━

  row_sim                         row_sim
     │                               │
 ┌───┴───┐                           │
 │       │                           │
 ▼       ▼                           ▼
mean   topk(5)                      mean
 │       │                           │
 │       ▼                           │
 │     MLP                           │
 │       │                           │
 └───┬───┘                           │
     │                               │
     ▼                               ▼
row_score                        row_score

同样处理col_sim                  同样处理col_sim

     │                               │
     └───────────┬───────────────────┘
                 │
                 ▼
            Final Score
```

---

## 8. 训练流程时间轴

```
Epoch 1-29
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    Batch 1 → Batch 2 → ... → Batch N
       │
       ▼
┌──────────────────────────────────────────────────┐
│  数据加载 (DataLoader)                            │
│  ────────────────────                            │
│  - images: (B, 3, H, W)                          │
│  - captions: (B, L_s)                            │
│  - long_captions: (B, L_d)                       │
│  - lengths: (B,)                                 │
│  - long_lengths: (B,)                            │
│  - img_ids: (B,)                                 │
└──────────────────────────────────────────────────┘
       │
       ▼
┌──────────────────────────────────────────────────┐
│  前向传播                                         │
│  ────────                                        │
│  1. 特征编码 (ViT + BERT)        ~10ms          │
│  2. SDTPS选择+聚合               ~5ms           │
│  3. HRPA对齐 (循环B_t次)         ~15ms          │
│  4. 损失计算                     ~1ms           │
│                                                  │
│  Total Forward Time:             ~30ms/batch    │
└──────────────────────────────────────────────────┘
       │
       ▼
┌──────────────────────────────────────────────────┐
│  反向传播                                         │
│  ────────                                        │
│  1. loss.backward()              ~50ms          │
│  2. 梯度裁剪 (grad_clip=2.0)     ~1ms           │
│  3. optimizer.step()             ~5ms           │
│  4. optimizer.zero_grad()        ~1ms           │
│                                                  │
│  Total Backward Time:            ~60ms/batch    │
└──────────────────────────────────────────────────┘
       │
       ▼
   每500步验证
       │
       ▼
┌──────────────────────────────────────────────────┐
│  验证 (Validation)                               │
│  ────────────────                                │
│  1. encode_data() - 编码所有数据  ~30s           │
│  2. shard_attn_scores() - 计算相似度 ~60s        │
│  3. i2t + t2i - 计算Recall       ~1s            │
│                                                  │
│  Total Validation Time:          ~90s           │
└──────────────────────────────────────────────────┘
       │
       ▼
   保存最佳模型
       │
       ▼
    继续训练...

Epoch 1 (Warmup)
────────────────
- max_violation = False (使用mean loss)
- warmup_alpha = [0, 1] (第一个epoch线性增长)

Epoch 2-29
──────────
- max_violation = True (使用hard negative)
- warmup_alpha = 1.0

学习率衰减 (lr_schedules=[9, 15, 20, 25]):
────────────────────────────────────────
Epoch 0-8:   lr = 2e-4
Epoch 9-14:  lr = 2e-4 × 0.3 = 6e-5
Epoch 15-19: lr = 6e-5 × 0.3 = 1.8e-5
Epoch 20-24: lr = 1.8e-5 × 0.3 = 5.4e-6
Epoch 25-29: lr = 5.4e-6 × 0.3 = 1.6e-6
```

---

## 9. 模型推理流程

```
┌─────────────────────────────────────────────────────────────┐
│                检索任务: 给定query找最相关图像                  │
└─────────────────────────────────────────────────────────────┘

准备阶段 (encode_data):
━━━━━━━━━━━━━━━━━━━━

    测试集图像 (5000张)
          │
          ▼
    批量编码 (batch=32)
          │
          ▼
    img_embs_all
      (5000, N+1, C)
          │
          ▼
      保存到内存

    测试集文本 (25000条)  # 每图5条
          │
          ▼
    批量编码 (batch=32)
          │
          ▼
    cap_embs_all, cap_lens_all
      (25000, L_s, C), (25000,)
          │
          ▼
      保存到内存

检索阶段 (shard_attn_scores):
━━━━━━━━━━━━━━━━━━━━━━━━━━

    对每个图像 i:
        │
        ▼
    img_embs[i] (1, N+1, C)
        │
        ├──→ 分离[CLS]
        │
        ├──→ 计算s_im
        │
        └──→ 对每个文本j:
                │
                ▼
            计算s_st, s_dt
                │
                ▼
            TokenSparse
                │
                ▼
            TokenAggregation
                │
                ▼
            HRPA
                │
                ▼
            sims[i,j] = S(I_i, T_j)

    最终得到:
    sims (5000, 25000)

评估阶段:
━━━━━━━━

Text-to-Image (t2i):
    对每个文本j:
        sims[:, j]  # 所有图像与文本j的相似度
        └→ argsort(descending)
        └→ 检查ground truth在Top-K中

    计算: R@1, R@5, R@10

Image-to-Text (i2t):
    对每个图像i:
        sims[i, :]  # 图像i与所有文本的相似度
        └→ argsort(descending)
        └→ 检查5个ground truth在Top-K中

    计算: R@1, R@5, R@10

rSum = R@1_i2t + R@5_i2t + R@10_i2t
     + R@1_t2i + R@5_t2i + R@10_t2i
```

---

## 10. 性能提升分解图

```
┌─────────────────────────────────────────────────────────────┐
│        SEPS vs Baseline 性能提升来源分析                       │
└─────────────────────────────────────────────────────────────┘

Baseline (SCAN):                        rSum = 485.6
━━━━━━━━━━━━━
    所有patch参与对齐
    无patch选择
    LeakyReLU相似度

    ↓ +7.2 (1.5%)
    引入图像自注意力
    └→ 识别视觉显著patch

SCAN + s_im:                            rSum = 492.8
━━━━━━━━━━━━
    ↓ +14.5 (3.0%)
    引入稀疏文本引导
    └→ 基于caption选择相关patch

LAPS (稀疏+聚合):                        rSum = 507.3
━━━━━━━━━━━━━━━━
    ↓ +24.6 (4.9%)
    引入MLLM稠密文本
    └→ 更丰富的语义指导

D2S-VSE (稠密文本蒸馏):                   rSum = 531.9
━━━━━━━━━━━━━━━━━━━
    ↓ +29.0 (5.5%)
    联合稀疏+稠密文本选择
    + 双分支聚合
    + HRPA增强对齐

SEPS (完整版):                          rSum = 560.9
━━━━━━━━━━━━
    总提升: +75.3 (15.5%)

贡献分解:
━━━━━━━
┌────────────────────────────────────────────┐
│ 稠密文本指导     +39.1 (52%)  ████████████ │
│ Patch聚合        +15.2 (20%)  █████        │
│ 双向对齐HRPA     +11.4 (15%)  ███          │
│ 图像自注意力     +7.2 (10%)   ██           │
│ 其他优化         +2.4 (3%)    █            │
└────────────────────────────────────────────┘

Text-to-Image提升最显著:
━━━━━━━━━━━━━━━━━━━━

SCAN:      56.4
SEPS:      86.9
提升:      +30.5 (54%)

原因:
- 稠密文本提供更丰富的检索线索
- 精准patch选择减少视觉噪声
- 双向对齐强化关键匹配
```

---

**文档生成**: 2025-12-04
**框架版本**: SEPS (ICLR 2026)
